{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "QUESTION 1:\n",
        "What is a Decision Tree, and how does it work in the context of classification?\n",
        "\n",
        "ANSWER:\n",
        "A Decision Tree is a supervised machine learning algorithm used for classification and regression.\n",
        "In classification, it works by recursively splitting the dataset based on feature values that best separate the target classes.\n",
        "Each internal node represents a feature test, each branch represents an outcome of the test, and each leaf node represents a class label.\n",
        "The tree makes decisions by following a path from the root to a leaf based on feature conditions.\n"
      ],
      "metadata": {
        "id": "9K2H8-EiLCo7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUESTION 2:\n",
        "Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "\n",
        "ANSWER:\n",
        "Gini Impurity measures how often a randomly chosen element would be incorrectly classified.\n",
        "Formula: Gini = 1 - Σ(p_i)^2\n",
        "\n",
        "Entropy measures the level of disorder or uncertainty in the dataset.\n",
        "Formula: Entropy = -Σ(p_i * log2(p_i))\n",
        "\n",
        "Lower values indicate purer nodes.\n",
        "Decision Trees choose splits that minimize Gini or Entropy, resulting in more homogeneous child nodes.\n",
        "Better impurity reduction leads to better splits.\n"
      ],
      "metadata": {
        "id": "ZpwmzlBFLF_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUESTION 3:\n",
        "What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "\n",
        "ANSWER:\n",
        "Pre-Pruning stops the tree from growing early by setting constraints like max_depth or min_samples_split.\n",
        "Advantage: Prevents overfitting and reduces training time.\n",
        "\n",
        "Post-Pruning allows the tree to grow fully and then removes unnecessary branches.\n",
        "Advantage: Produces a simpler model with better generalization.\n"
      ],
      "metadata": {
        "id": "swzuh8lnLGbC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUESTION 4:\n",
        "What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "\n",
        "ANSWER:\n",
        "Information Gain measures the reduction in entropy after a dataset is split on a feature.\n",
        "Formula: Information Gain = Entropy(parent) - Σ(weighted Entropy(children))\n",
        "\n",
        "It is important because it helps select the feature that best separates the data into pure classes.\n",
        "Higher Information Gain results in better splits.\n"
      ],
      "metadata": {
        "id": "3zLMGGQ1LGtD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUESTION 5:\n",
        "What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "\n",
        "ANSWER:\n",
        "Applications:\n",
        "- Medical diagnosis\n",
        "- Credit risk assessment\n",
        "- Fraud detection\n",
        "- Customer churn prediction\n",
        "\n",
        "Advantages:\n",
        "- Easy to understand and interpret\n",
        "- Handles both numerical and categorical data\n",
        "- Requires minimal data preprocessing\n",
        "\n",
        "Limitations:\n",
        "- Prone to overfitting\n",
        "- Sensitive to small changes in data\n",
        "- Less accurate compared to ensemble methods\n"
      ],
      "metadata": {
        "id": "gK0LA0k-LG_C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUESTION 6:\n",
        "Python program to load Iris dataset, train Decision Tree (Gini), print accuracy and feature importances\n"
      ],
      "metadata": {
        "id": "Dlfhq3cnLXVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = DecisionTreeClassifier(criterion=\"gini\")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Feature Importances:\", model.feature_importances_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzeQMgX6LUSj",
        "outputId": "b6f4f5df-8e18-469a-f9bf-d81097af9c6f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Feature Importances: [0.         0.01667014 0.90614339 0.07718647]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUESTION 7:\n",
        "Train Decision Tree with max_depth=3 and compare with fully grown tree\n"
      ],
      "metadata": {
        "id": "xiwtJRMNLfBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "full_tree = DecisionTreeClassifier()\n",
        "limited_tree = DecisionTreeClassifier(max_depth=3)\n",
        "\n",
        "full_tree.fit(X_train, y_train)\n",
        "limited_tree.fit(X_train, y_train)\n",
        "\n",
        "print(\"Full Tree Accuracy:\", accuracy_score(y_test, full_tree.predict(X_test)))\n",
        "print(\"Max Depth=3 Accuracy:\", accuracy_score(y_test, limited_tree.predict(X_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woWwiSlDLZO3",
        "outputId": "9bb542f1-844e-4d4c-d05e-abdc5842131b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full Tree Accuracy: 1.0\n",
            "Max Depth=3 Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUESTION 8:\n",
        "Train Decision Tree Regressor on Boston Housing dataset and print MSE and feature importances\n"
      ],
      "metadata": {
        "id": "Y8bRIRggLn1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = DecisionTreeRegressor()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n",
        "print(\"Feature Importances:\", model.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mj9C1M4cLg7O",
        "outputId": "04697a1b-6cbc-4294-cbdb-ab1e6276ce43"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.48981034170457843\n",
            "Feature Importances: [0.52817895 0.05195682 0.05410209 0.02902475 0.02977124 0.13098161\n",
            " 0.09300318 0.08298136]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUESTION 9:\n",
        "Hyperparameter tuning using GridSearchCV\n"
      ],
      "metadata": {
        "id": "8sSilKODL9ya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    \"max_depth\": [2, 3, 4, 5],\n",
        "    \"min_samples_split\": [2, 5, 10]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Accuracy:\", accuracy_score(y_test, best_model.predict(X_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhk_Dl9ZLpeR",
        "outputId": "553673b0-a0d8-484c-fd88-cc4ad2dd5ff3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Best Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUESTION 10:\n",
        "Healthcare Decision Tree workflow and business value\n",
        "\n",
        "ANSWER:\n",
        "Step 1: Handle missing values using mean/median for numerical data and mode for categorical data.\n",
        "Step 2: Encode categorical features using Label Encoding or One-Hot Encoding.\n",
        "Step 3: Split the dataset into training and testing sets.\n",
        "Step 4: Train a Decision Tree classifier on the processed data.\n",
        "Step 5: Tune hyperparameters like max_depth and min_samples_split using GridSearchCV.\n",
        "Step 6: Evaluate performance using accuracy, precision, recall, F1-score, and confusion matrix.\n",
        "\n",
        "Business Value:\n",
        "The model helps in early disease detection, reduces diagnostic costs, improves treatment planning, and supports doctors in decision-making.\n"
      ],
      "metadata": {
        "id": "90x6K1lbMBZd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "duUKRRUcMB2F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}